## 感知机的对偶形式

感知机的**对偶形式**是感知机算法的一种改写形式，它通过利用样本点之间的内积计算来更新模型参数。这种形式避免了直接存储和更新权重向量 \( \mathbf{w} \)，而是用样本点和其分类误差累积来表示模型参数，具有重要的理论意义。以下是对偶形式的详细讲解。

---

### 1. **感知机的原始形式**
感知机的原始形式更新规则为：
\[
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i
\]
\[
b \leftarrow b + \eta y_i
\]
其中：
- \( \mathbf{w} \) 是权重向量。
- \( b \) 是偏置。
- \( \eta \) 是学习率。
- \( y_i \) 是样本的真实标签（取值为 \(-1\) 或 \(+1\)）。
- \( \mathbf{x}_i \) 是样本特征向量。

更新后，模型的预测规则为：
\[
f(\mathbf{x}) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b)
\]

---

### 2. **对偶形式的思想**
对偶形式的关键在于将权重向量 \( \mathbf{w} \) 用训练样本的线性组合表示：
\[
\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i
\]
其中：
- \( \alpha_i \) 是样本 \( \mathbf{x}_i \) 对权重的贡献系数，初始值为 0，在训练过程中更新。
- \( y_i \) 是样本标签。

---

### 3. **对偶形式的推导**
#### 权重的表示
感知机的更新规则为：
\[
\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i
\]
将权重的更新展开为所有误分类样本的累积：
\[
\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i
\]
其中 \( \alpha_i \) 是样本 \( \mathbf{x}_i \) 被更新的次数，即：
\[
\alpha_i \leftarrow \alpha_i + \eta
\]

#### 偏置的表示
类似地，偏置 \( b \) 也根据误分类样本的累积更新为：
\[
b = \sum_{i=1}^N \alpha_i y_i
\]

#### 模型的预测函数
将 \( \mathbf{w} \) 的表示代入预测公式：
\[
f(\mathbf{x}) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b)
\]
代入 \( \mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i \) 后，有：
\[
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^N \alpha_i y_i (\mathbf{x}_i \cdot \mathbf{x}) + \sum_{i=1}^N \alpha_i y_i \right)
\]

#### 核心特性
在对偶形式中，模型的学习和预测仅依赖于样本点之间的内积 \( \mathbf{x}_i \cdot \mathbf{x}_j \)，而不需要直接计算权重 \( \mathbf{w} \)。这为扩展到非线性分类问题（使用核方法）提供了基础。

---

### 4. **对偶形式的更新规则**
对偶形式的算法更新规则为：
1. 初始化：\( \alpha_i = 0 \)，\( b = 0 \)。
2. 对于每个样本 \( \mathbf{x}_i \)，检查是否误分类：
   \[
   y_i \left(\sum_{j=1}^N \alpha_j y_j (\mathbf{x}_j \cdot \mathbf{x}_i) + b \right) \leq 0
   \]
3. 如果误分类，则更新：
   \[
   \alpha_i \leftarrow \alpha_i + \eta
   \]
   \[
   b \leftarrow b + \eta y_i
   \]

---

### 5. **对偶形式的优势**
- **避免直接存储权重**：使用 \( \alpha_i \) 和内积表示权重，方便处理高维数据。
- **扩展到核方法**：由于模型仅依赖样本点之间的内积 \( \mathbf{x}_i \cdot \mathbf{x}_j \)，可以将内积替换为核函数 \( K(\mathbf{x}_i, \mathbf{x}_j) \) 来解决非线性问题。
- **理论价值**：对偶形式揭示了感知机算法的本质：模型通过支持样本的线性组合来构造决策边界。

---

### 6. **局限性**
- **计算效率**：对偶形式需要存储和计算所有样本的内积，对于大数据集会增加计算成本。
- **线性可分性**：感知机仍然只能解决线性可分问题。


在机器学习和信号处理中，**Gram 矩阵** 是一个对称矩阵，描述了一组向量之间的内积关系。它在核方法（如支持向量机、核PCA）以及感知机的对偶形式中具有重要作用。

## Gram 矩阵

### 1. **Gram 矩阵的定义**
给定 \( n \) 个向量 \( \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \)（这些向量通常来自于输入空间），Gram 矩阵 \( G \) 是一个 \( n \times n \) 的矩阵，其元素由以下公式定义：
\[
G_{ij} = \mathbf{x}_i \cdot \mathbf{x}_j
\]
其中：
- \( \mathbf{x}_i \cdot \mathbf{x}_j \) 是向量 \( \mathbf{x}_i \) 和 \( \mathbf{x}_j \) 的内积。

因此，Gram 矩阵可以写作：
\[
G = 
\begin{bmatrix}
\mathbf{x}_1 \cdot \mathbf{x}_1 & \mathbf{x}_1 \cdot \mathbf{x}_2 & \cdots & \mathbf{x}_1 \cdot \mathbf{x}_n \\
\mathbf{x}_2 \cdot \mathbf{x}_1 & \mathbf{x}_2 \cdot \mathbf{x}_2 & \cdots & \mathbf{x}_2 \cdot \mathbf{x}_n \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{x}_n \cdot \mathbf{x}_1 & \mathbf{x}_n \cdot \mathbf{x}_2 & \cdots & \mathbf{x}_n \cdot \mathbf{x}_n
\end{bmatrix}
\]

---

### 2. **特性**
1. **对称性**：
   Gram 矩阵是对称的，因为内积满足交换律：
   \[
   G_{ij} = \mathbf{x}_i \cdot \mathbf{x}_j = \mathbf{x}_j \cdot \mathbf{x}_i
   \]

2. **正定性**：
   Gram 矩阵是半正定的，即对于任何非零向量 \( \mathbf{a} \)，有：
   \[
   \mathbf{a}^\top G \mathbf{a} \geq 0
   \]
   如果 \( \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \) 线性无关，则 \( G \) 是正定的。

3. **核方法的核心**：
   在核方法中，Gram 矩阵的元素可以通过核函数 \( K(\mathbf{x}_i, \mathbf{x}_j) \) 计算：
   \[
   G_{ij} = K(\mathbf{x}_i, \mathbf{x}_j)
   \]
   例如，若使用多项式核 \( K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + c)^d \)，则 Gram 矩阵的元素表示经过核变换后的内积。

---

### 3. **在感知机对偶形式中的应用**
在感知机的对偶形式中，权重向量 \( \mathbf{w} \) 用训练样本的线性组合表示：
\[
\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i
\]
预测函数则可以写作：
\[
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^N \alpha_i y_i (\mathbf{x}_i \cdot \mathbf{x}) + b \right)
\]
其中，内积 \( \mathbf{x}_i \cdot \mathbf{x}_j \) 构成了 Gram 矩阵。

Gram 矩阵允许我们高效计算所有样本点之间的内积，避免显式计算权重向量。

---




### 4. **总结**
Gram 矩阵是描述样本间关系的重要工具：
- 在对偶形式中，它通过内积简化权重更新。
- 在核方法中，它是高维特征映射后的相似性矩阵。
- 它为很多机器学习算法（如支持向量机、核PCA）提供了理论基础和计算手段。